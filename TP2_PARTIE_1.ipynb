{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2. PARTIE 1. scikit-learn + Naive Bayes\n",
    "\n",
    "<img src=\"http://media.giphy.com/media/2lbhL8dSGMh8I/giphy.gif\"  width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module de Machine Learning en Python : scitkit-learn (sklearn)\n",
    "http://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    "\n",
    "   [- Iris dataset](#1)\n",
    "   \n",
    "   [- Naive Bayes](#2)\n",
    "   \n",
    "   [- Mon Naive Bayes](#3)\n",
    "   \n",
    "   [- Tests](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://78.media.tumblr.com/ec97315a70ebd605ac05f3b91feadaa5/tumblr_monmpxBfcM1qirapio1_500.gif\" width = 200>\n",
    "<a id=\"1\"></a> \n",
    " \n",
    "# 1. Iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher le dataset **iris** dans le module sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://media.giphy.com/media/c1zviFHCf4pq0/giphy.gif\" width = 200>\n",
    "<a id=\"2\"></a> \n",
    " \n",
    "# 2. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(BaseNB)\n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via `partial_fit` method.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like, shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_prior_ : array, shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  theta_ : array, shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  sigma_ : array, shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, priors=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module sklearn.naive_bayes:\n",
      "\n",
      "fit(X, y, sample_weight=None) method of sklearn.naive_bayes.GaussianNB instance\n",
      "    Fit Gaussian Naive Bayes according to X, y\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vectors, where n_samples is the number of samples\n",
      "        and n_features is the number of features.\n",
      "    \n",
      "    y : array-like, shape (n_samples,)\n",
      "        Target values.\n",
      "    \n",
      "    sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "        Weights applied to individual samples (1. for unweighted).\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Returns self.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "help(gnb.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31,  38, 126,  26,  22,  37,  19,  87,  82,  95,  35, 122, 104,\n",
       "        16,  78,   7,  21,  36,  24,  44,  69, 108, 123,  55,   9,  40,\n",
       "        14,   6,   2, 128,  59,  41, 143,  91,  46, 145, 133,  11,  76,\n",
       "        39,  80,  65, 142,  29,  99, 141,  84,  81,  94, 127,  52, 117,\n",
       "        27, 121,  51, 139,  83, 114,  73,   0, 144, 147,  97,  96,  33,\n",
       "        86,  77,  57, 120,  23,   1, 103,  54,  53, 109])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.random.choice(range(150), 75, replace=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = range(150)\n",
    "test = np.delete(a, train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08461538461538462"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.random.choice(range(150), 20, replace=False)\n",
    "a = range(150)\n",
    "test = np.delete(a, train)\n",
    "y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "(iris.target[test,] != y_pred).sum()/len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB(A,B,nb):\n",
    "    res = []\n",
    "    for i in range(B-A+1):\n",
    "        temp = 0\n",
    "        for j in range(nb):\n",
    "            train = np.random.choice(range(150), A+i, replace=False)\n",
    "            a = range(150)\n",
    "            test = np.delete(a, train)\n",
    "            y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "            temp = temp +(iris.target[test,] != y_pred).sum()/len(test)\n",
    "        res = res + [100*temp/nb]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d1d6d37828>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEyCAYAAADeAVWKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1d6cda630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = 2\n",
    "B = 149\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(A,B,B-A+1), NB(A,B,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://media3.giphy.com/media/KYMR9VsDGs3PW/giphy.gif\" width = 200>\n",
    "<a id=\"3\"></a> \n",
    " \n",
    "# 3. Mon Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MonNaiveBayes(train, test):\n",
    "    #TARGET\n",
    "    unique, counts = np.unique(train.target, return_counts=True)\n",
    "    freq = counts/len(train.target)\n",
    "    dic = dict(zip(unique, freq)) \n",
    "    estimM = np.zeros(shape=(len(unique),train.data.shape[1]))\n",
    "    estimV = np.zeros(shape=(len(unique),train.data.shape[1])) \n",
    "    #DATA : estimation\n",
    "    i = 0\n",
    "    for tar in unique:\n",
    "        estimM[i,] = np.apply_along_axis(np.mean, 0, train.data[train.target == tar,])\n",
    "        estimV[i,] = np.apply_along_axis(np.var, 0, train.data[train.target == tar,])\n",
    "        i = i+1\n",
    "\n",
    "    choice = np.zeros(shape=(test.data.shape[0],len(unique)))\n",
    "    #DATA : test \n",
    "    for i in range(test.data.shape[0]):\n",
    "        for j in range(len(unique)):\n",
    "            choice[i,j] = -0.5*sum((test.data[i]-estimM[j])*(test.data[i]-estimM[j])/estimV[j]) + np.log(freq[j]) \n",
    "    res = np.argmax(choice, axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MonNaiveBayes(iris, iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "y_pred == MonNaiveBayes(iris, iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "iris2 = copy.deepcopy(iris)\n",
    "\n",
    "iris2.data = iris2.data[3:30,]\n",
    "iris2.target = iris2.target[3:30,]\n",
    "MonNaiveBayes(iris, iris2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://media2.giphy.com/media/GgtcDmMa97FhC/200.gif\" width = 200>\n",
    "<a id=\"3\"></a> \n",
    " \n",
    "# 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
